{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from tkinter.filedialog import askdirectory\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from scipy.integrate import trapezoid\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "from numpy.fft import fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Quantacell\n",
    "\n",
    "def filter_peaks_by_shape(peaks, signal, min_slope=5, window=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Filters peaks based on their shape by analyzing slopes and width.\n",
    "\n",
    "    :param peaks: np.ndarray, indices of detected peaks.\n",
    "    :param signal: np.ndarray, 1D signal.\n",
    "    :param min_slope: float, minimum slope to consider a peak as sharp.\n",
    "    :param window: int, window size.\n",
    "    :return: np.ndarray, indices of filtered peaks.\n",
    "    \"\"\"\n",
    "    filtered_peaks, filtered_peaks_indexes = [], []\n",
    "    for k, peak in enumerate(peaks):\n",
    "        # if peak <= (window // 2) or peak == len(signal) - (window // 2):\n",
    "        if peak == 0 or peak == len(signal) - 1:\n",
    "            if verbose:\n",
    "                print(f'Peak {peak} ignored (signal edge).')\n",
    "            continue\n",
    "        \n",
    "        start_left = max(0, peak - window)\n",
    "        end_left = peak\n",
    "        start_right = peak + 1\n",
    "        end_right = min(len(signal), peak + 1 + window)\n",
    "        \n",
    "        # Compute slope on the left and on the right\n",
    "        left_slope = (signal[peak] - np.mean(signal[start_left:end_left])) / (end_left - start_left)\n",
    "        right_slope = (signal[peak] - np.mean(signal[start_right:end_right])) / (end_right - start_right)\n",
    "        \n",
    "        if left_slope < min_slope or right_slope < min_slope:\n",
    "            if verbose:\n",
    "                print(f'Peak {peak} ignored (low slope): ls = {left_slope} rs = {right_slope}')\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Peak {peak} ignored (low slope): ls = {left_slope} rs = {right_slope}')\n",
    "        filtered_peaks_indexes.append(k)\n",
    "        filtered_peaks.append(peak)\n",
    "    \n",
    "    return np.array(filtered_peaks), np.array(filtered_peaks_indexes)\n",
    "\n",
    "\n",
    "def filter_by_duration_and_calculate(peaks, threshold, signal, sample_rate=30,\n",
    "                                     min_duration=0.1, min_flow=-1, verbose=False):\n",
    "    \"\"\"\n",
    "    Filters peaks based on their duration above a threshold, computes the area\n",
    "    of each expansion above the threshold.\n",
    "    \n",
    "    :param peaks: np.ndarray, indices of detected peaks.\n",
    "    :param threshold: float, threshold.\n",
    "    :param signal: np.ndarray, 1D signal.\n",
    "    :param sample_rate: int, sampling rate of the signal.\n",
    "    :param min_duration: float, minimum duration in seconds.\n",
    "    :param min_flow: float, minimum flow of expansion.\n",
    "    :return: tuple, filtered peaks, their durations, their areas and flows.\n",
    "    \"\"\"\n",
    "    valid_peaks, durations, areas, flows = [], [], [], []\n",
    "    sample_rate = max(1e-6, sample_rate) # preventing zero division\n",
    "    for peak in peaks:\n",
    "        peak_time = np.round(peak / sample_rate, 3)\n",
    "        start = peak\n",
    "        while start > 0 and signal[start] > threshold:\n",
    "            start -= 1\n",
    "        end = peak\n",
    "        while end < len(signal) and signal[end] > threshold:\n",
    "            end += 1\n",
    "        end = min(end, len(signal) - 1)\n",
    "        duration = (end - start) / sample_rate\n",
    "        segment_times = np.arange(start, end + 1)\n",
    "        area = np.abs(trapezoid(np.maximum(signal[start:end+1] - threshold, 0), segment_times)).astype(int)\n",
    "        area_flow = area * duration\n",
    "        if duration >= min_duration and area_flow >= min_flow:\n",
    "            valid_peaks.append(peak)\n",
    "            durations.append(duration)\n",
    "            areas.append(area)\n",
    "            flows.append(area_flow)\n",
    "        elif verbose:\n",
    "            print(f'Peak {peak} at {peak_time}s ignored (not last long enough): duration = {duration} flow = {area_flow}')\n",
    "    return np.array(valid_peaks), durations, areas, flows\n",
    "\n",
    "\n",
    "def filter_close_peaks(peaks, signal, min_distance, verbose=False):\n",
    "    \"\"\"\n",
    "    Removes peaks that are too close to each other.\n",
    "\n",
    "    :param peaks: np.ndarray, indices of detected peaks.\n",
    "    :param signal: np.ndarray, 1D signal.\n",
    "    :param durations: list, durations of each peak.\n",
    "    :param min_distance: int, minimum allowed distance between peaks.\n",
    "    :return: tuple, filtered peaks and durations.\n",
    "    \"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, []\n",
    "    filtered_peaks = [peaks[0]]\n",
    "    filtered_peaks_indexes = [0]\n",
    "    # filtered_durations = [durations[0]]\n",
    "    for i in range(1, len(peaks)):\n",
    "        if peaks[i] - filtered_peaks[-1] >= min_distance:\n",
    "            filtered_peaks.append(peaks[i])\n",
    "            filtered_peaks_indexes.append(i)\n",
    "            # filtered_durations.append(durations[i])\n",
    "        else:\n",
    "            if signal[peaks[i]] > signal[filtered_peaks[-1]]:\n",
    "                if verbose:\n",
    "                    print(f'Peak {peaks[i]} removed (too close from {filtered_peaks[-1]}).')\n",
    "                filtered_peaks[-1] = peaks[i]\n",
    "                filtered_peaks_indexes[-1] = i\n",
    "                # filtered_durations[-1] = durations[i]\n",
    "    return np.array(filtered_peaks), filtered_peaks_indexes\n",
    "\n",
    "\n",
    "def filter_edge_peaks(peaks, signal_length, min_distance_from_edges, verbose=False):\n",
    "    \"\"\"\n",
    "    Filtre les pics proches des extrémités du signal.\n",
    "    \n",
    "    :param peaks: np.ndarray, indices des pics détectés.\n",
    "    :param signal_length: int, longueur totale du signal.\n",
    "    :param min_distance_from_edges: int, distance minimale des extrémités.\n",
    "    :return: np.ndarray, indices des pics après filtrage.\n",
    "    \"\"\"\n",
    "    filetered_peaks = []\n",
    "    for peak in peaks:\n",
    "        if peak >= min_distance_from_edges and peak <= (signal_length - min_distance_from_edges):\n",
    "            filetered_peaks.append(peak)\n",
    "        elif verbose:\n",
    "            print(f'Peak {peak} removed (too close from edges).')\n",
    "    return np.array(filetered_peaks)\n",
    "    # return np.array([peak for peak in peaks if peak >= min_distance_from_edges and peak <= (signal_length - min_distance_from_edges)])\n",
    "\n",
    "\n",
    "\n",
    "def analyze_signal_peaks(signal, prct_1=10, prct_2=90, sample_rate=30,\n",
    "                         min_duration=0.1, min_distance=10,\n",
    "                         min_flow=1000, min_slope=10, window_slope_size=10,\n",
    "                         verbose=False):\n",
    "    \"\"\"\n",
    "    Analyzes a 1D signal to detect and filter high and low peaks based on various thresholds and parameters.\n",
    "    It identifies peaks, filters them by duration, distance, and shape, and computes additional properties such as\n",
    "    the area and flow for each peak.\n",
    "\n",
    "    :param signal: np.ndarray\n",
    "        A 1D numpy array representing the signal to analyze.\n",
    "\n",
    "    :param prct_1: int, optional, default=10\n",
    "        The percentile value for detecting low peaks. Values below this percentile are considered low peaks.\n",
    "\n",
    "    :param prct_2: int, optional, default=90\n",
    "        The percentile value for detecting high peaks. Values above this percentile are considered high peaks.\n",
    "\n",
    "    :param sample_rate: int, optional, default=30\n",
    "        The sample rate of the signal, which is used to convert durations and distances from indices to seconds.\n",
    "\n",
    "    :param min_duration: float, optional, default=0.1\n",
    "        The minimum duration (in seconds) that a peak must last in order to be considered significant. \n",
    "\n",
    "    :param min_distance: int, optional, default=10\n",
    "        The minimum distance between consecutive peaks (in indices). Peaks that are too close together are filtered out.\n",
    "\n",
    "    :param min_flow: float, optional, default=1000\n",
    "        The minimum flow, representing the strength of a peak's contraction or expansion, calculated as the area between\n",
    "        the peak and its threshold.\n",
    "\n",
    "    :param min_slope: float, optional, default=10\n",
    "        The minimum slope required for the peak's shape to be considered valid.\n",
    "\n",
    "    :param window_slope_size: int, optional, default=10\n",
    "        The window size used to calculate the slope to the left and right of each peak.\n",
    "\n",
    "    :param verbose: bool, optional, default=False\n",
    "        If set to True, additional information will be printed during the processing steps.\n",
    "\n",
    "    \"\"\"\n",
    "    assert prct_1 < prct_2\n",
    "    low_percentile = np.percentile(signal, prct_1)\n",
    "    high_percentile = np.percentile(signal, prct_2)\n",
    "    MH_highpeaks=0.5*np.max(signal)\n",
    "    MH_lowpeaks=np.min(signal)+0.2*np.max(signal)\n",
    "    high_peaks, _ = find_peaks(signal, height=MH_highpeaks)\n",
    "    low_peaks, _ = find_peaks(-signal, height=-MH_lowpeaks)\n",
    "\n",
    "    high_peaks = filter_edge_peaks(high_peaks, len(signal), min_distance_from_edges=5)\n",
    "    low_peaks = filter_edge_peaks(low_peaks, len(signal), min_distance_from_edges=5)\n",
    "\n",
    "    high_peaks, high_durations, high_areas, high_flows = filter_by_duration_and_calculate(high_peaks, MH_highpeaks,\n",
    "                                                                  signal, sample_rate,\n",
    "                                                                  min_duration, min_flow, verbose)\n",
    "    low_peaks, low_durations, low_areas, low_flows = filter_by_duration_and_calculate(low_peaks, -MH_lowpeaks,\n",
    "                                                                -signal, sample_rate,\n",
    "                                                                min_duration, min_flow, verbose)\n",
    "    # Filter close peak\n",
    "    min_distance_in_idx = int(np.round(min_distance * sample_rate))\n",
    "    high_peaks, high_indexes = filter_close_peaks(high_peaks, signal,\n",
    "                                                    min_distance_in_idx, verbose)\n",
    "    low_peaks, low_indexes = filter_close_peaks(low_peaks, -signal,\n",
    "                                                  min_distance_in_idx, verbose)\n",
    "    high_durations = [high_durations[idx] for idx in high_indexes if idx in high_indexes]\n",
    "    low_durations = [low_durations[idx] for idx in low_indexes if idx in low_indexes]\n",
    "    high_areas = [high_areas[idx] for idx in high_indexes if idx in high_indexes]\n",
    "    low_areas = [low_areas[idx] for idx in low_indexes if idx in low_indexes]\n",
    "    high_flows = [high_flows[idx] for idx in high_indexes if idx in high_indexes]\n",
    "    low_flows = [low_flows[idx] for idx in low_indexes if idx in low_indexes]\n",
    "    \n",
    "    # Filter by shape\n",
    "    high_peaks, high_indexes = filter_peaks_by_shape(high_peaks, signal,\n",
    "                                                     min_slope, window_slope_size,\n",
    "                                                     verbose=verbose)\n",
    "    low_peaks, low_indexes = filter_peaks_by_shape(low_peaks, -signal,\n",
    "                                                   min_slope, window_slope_size,\n",
    "                                                   verbose=verbose)\n",
    "    high_durations = [high_durations[idx] for idx in high_indexes if idx in high_indexes]\n",
    "    low_durations = [low_durations[idx] for idx in low_indexes if idx in low_indexes]\n",
    "    high_areas = [high_areas[idx] for idx in high_indexes if idx in high_indexes]\n",
    "    low_areas = [low_areas[idx] for idx in low_indexes if idx in low_indexes]\n",
    "    high_flows = [high_flows[idx] for idx in high_indexes if idx in high_indexes]\n",
    "    low_flows = [low_flows[idx] for idx in low_indexes if idx in low_indexes]\n",
    "    \n",
    "    # Résultats\n",
    "    results = {\n",
    "        \"low_percentile_value\": prct_1,\n",
    "        \"high_percentile_value\": prct_2,\n",
    "        \"low_percentile\": low_percentile,\n",
    "        \"high_percentile\": high_percentile,\n",
    "        \"high_amplitude\": MH_highpeaks,\n",
    "        \"low_amplitude\": MH_lowpeaks,\n",
    "        \"high_peak_indices\": high_peaks.tolist(),\n",
    "        \"low_peak_indices\": low_peaks.tolist(),\n",
    "        \"high_peak_durations\": high_durations,\n",
    "        \"low_peak_durations\": low_durations,\n",
    "        'high_areas': high_areas,\n",
    "        'low_areas': low_areas,\n",
    "        'high_flows': high_flows,\n",
    "        'low_flows': low_flows,\n",
    "        \"total_number_of_peaks\": len(low_peaks) + len(high_peaks)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_signal_with_filtered_peaks(signal, sample_rate, results, title=None):\n",
    "    \"\"\"\n",
    "    Plots the signal with detected and filtered high and low peaks.\n",
    "\n",
    "    :param signal: np.ndarray, 1D signal.\n",
    "    :param sample_rate: int, signal sampling rate.\n",
    "    :param results: dict, results of peak analysis.\n",
    "    :param title: str, optional, plot title.\n",
    "    \"\"\"\n",
    "    time = np.arange(len(signal)) / sample_rate\n",
    "    \n",
    "    prct_1_value = results['low_percentile_value']\n",
    "    prct_2_value = results['high_percentile_value']\n",
    "    high_peaks = results[\"high_peak_indices\"]\n",
    "    low_peaks = results[\"low_peak_indices\"]\n",
    "    low_percentile = results[\"low_percentile\"]\n",
    "    high_percentile = results[\"high_percentile\"]\n",
    "    low_amplitude = results[\"low_amplitude\"]\n",
    "    high_amplitude = results[\"high_amplitude\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time, signal, label=\"Signal\", color=\"blue\", linewidth=1.5)\n",
    "\n",
    "    plt.axhline(high_percentile, color=\"green\", linestyle=\"--\", label=f\"{prct_1_value} percentile\")\n",
    "    plt.axhline(low_percentile, color=\"red\", linestyle=\"--\", label=f\"{prct_2_value} percentile\")\n",
    "\n",
    "    if len(high_peaks) > 0:\n",
    "        plt.scatter(time[high_peaks], signal[high_peaks], color=\"orange\", label=\"High peak(s)\", zorder=5)\n",
    "    \n",
    "    if len(low_peaks) > 0:\n",
    "        plt.scatter(time[low_peaks], signal[low_peaks], color=\"purple\", label=\"Low peak(s)\", zorder=5)\n",
    "\n",
    "    if title is None:\n",
    "        title = \"Signal avec Pics Hauts et Bas Filtrés\"\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Time (s)\", fontsize=12)\n",
    "    plt.ylabel(\"Amplitude\", fontsize=12)\n",
    "    plt.legend(loc=\"best\", fontsize=10)\n",
    "    plt.grid(alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def signaltonoise(a, axis=0, ddof=0):\n",
    "    '''Signal to noise ratio of signal a.\n",
    "    Parameters:\n",
    "    a is an array\n",
    "    '''\n",
    "    a = np.asanyarray(a)\n",
    "    m = a.mean(axis)\n",
    "    sd = a.std(axis=axis, ddof=ddof)\n",
    "    return np.where(sd == 0, 0, m/sd)\n",
    "\n",
    "# Mathilde's function\n",
    "\n",
    "\n",
    "def sort_peaks_troughs(signal,peaks, all_troughs, Window):\n",
    "    '''\n",
    "    Sorts peaks and troughs and checks if they match.\n",
    "    Parameters:\n",
    "    peaks (list or array-like): Indices of the peaks in the signal.\n",
    "    all_troughs (list or array-like): Indices of all troughs in the signal.\n",
    "    Returns:\n",
    "    tuple: Peaks, pretroughs, troughs, passs, message.\n",
    "    Peaks, pretroughs and troughs are lists of indices.\n",
    "    Passs is a boolean indicating if the signal should be skipped.\n",
    "    Message is a string with a reason for skipping the signal.\n",
    "    '''\n",
    "\n",
    "    filtered_troughs = []\n",
    "    last_troughs = []  # list to store last min after each peak\n",
    "    first_troughs = all_troughs[all_troughs < peaks[0]]   # First, check for troughs before the first peak \n",
    "    dSignal=np.diff(signal)\n",
    "    epsilon=max(dSignal)*0.005\n",
    "\n",
    "    if len(first_troughs) > 0:\n",
    "        last_troughs.append(first_troughs[-1])  # add the closest trough to first peak to last_troughs\n",
    "\n",
    "    for i in range(len(peaks)):\n",
    "        # find all troughs after this peak and before next peak\n",
    "        if i == len(peaks) - 1: next_troughs = all_troughs[all_troughs > peaks[i]] # for the peak before the last one\n",
    "        else: next_troughs = all_troughs[(all_troughs > peaks[i]) & (all_troughs < peaks[i+1])] #take all the troughs between peak i and i+1\n",
    "        if len(next_troughs) > 0:     # if there are multiple troughs after a peak or before first peak, keep only the closest ones\n",
    "            find_trough=0\n",
    "            T_cursor=0\n",
    "            while find_trough==0  and len(next_troughs)-1>T_cursor:\n",
    "                if np.mean(dSignal[next_troughs[T_cursor]:next_troughs[T_cursor+1]])<-epsilon:T_cursor+=1 #Trough should be at the end of the peak, where derivative is very small\n",
    "                else:find_trough=1         \n",
    "                \n",
    "            filtered_troughs.append(next_troughs[T_cursor])\n",
    "            last_troughs.append(next_troughs[-1]) # keep the last one for last_troughs list\n",
    "\n",
    "    troughs = np.array(filtered_troughs)\n",
    "    pretroughs = np.array(last_troughs)\n",
    "\n",
    "    \n",
    "\n",
    "    if pretroughs[0] > peaks[0]: # Ensure the first beat starts with a pretrough\n",
    "        peaks = peaks[1:]\n",
    "    if len(peaks) < 2:\n",
    "        message = 'One or less peaks found. Onto the next signal.'\n",
    "        return None, None, None, True, message\n",
    "    \n",
    "    if len(troughs)==0 or len(pretroughs)==0:\n",
    "        message = 'One or less peaks found. Troughs or pretroughs=0. Onto the next signal.'\n",
    "        return None, None, None, True, message\n",
    "\n",
    "    if peaks[-1] > troughs[-1]: # Ensure the last peak ends with a trough\n",
    "        peaks = peaks[:-1]\n",
    "    if len(peaks) < 2:\n",
    "        message = 'One or less peaks found. Onto the next signal.'\n",
    "        return None, None, None, True, message\n",
    "\n",
    "    if pretroughs[-1] > peaks[-1]: # Ensure the last pretrough is before the last peak\n",
    "        pretroughs = pretroughs[:-1]\n",
    "    if troughs[0] < peaks[0]: # Ensure the first trough is after the first peak\n",
    "        troughs = troughs[1:]\n",
    "\n",
    "    if len(peaks) != len(troughs) or len(peaks) != len(pretroughs):     # if each peak doesn't have a pretrough before and a trough after, pass\n",
    "        message = 'Peaks, troughs and pretroughs do not match. Onto the next signal.'\n",
    "        return None, None, None, True, message\n",
    "    \n",
    "    return peaks, pretroughs, troughs, False, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ContractionPoints(i, CR_percents, peaks, strain_vector,time_vector, pretroughs, troughs):\n",
    "    '''\n",
    "    Function to compute contraction/relaxation time values for each percentage.\n",
    "    Parameters:\n",
    "    i (int): Index of the signal.\n",
    "    CR_percents (list): List of contraction and relaxation percentages to compute.\n",
    "    peaks (list): List of peak indices.\n",
    "    signal (DataFrame): DataFrame containing the signal data.\n",
    "    pretroughs (list): List of pretrough indices.\n",
    "    troughs (list): List of trough indices.\n",
    "    Returns:\n",
    "    tuple: relax, relax x, relax y, contrac x, contrac y.\n",
    "    relax, relax x, relax y are dictionaries containing the Strain values and corresponding times for each relaxation percentage.\n",
    "    contrac, contrac x, contrac y are dictionaries containing the Strain values and corresponding times for each contraction percentage.\n",
    "    '''\n",
    "    # Create lists to store all APD values\n",
    "    relax_x = {f'relax{p}_x': [] for p in CR_percents}\n",
    "    relax_y = {f'relax{p}_y': [] for p in CR_percents}\n",
    "    contrac = {f'contrac{p}': [] for p in CR_percents}\n",
    "    relax = {f'relax{p}': [] for p in CR_percents}\n",
    "    contrac_x = {f'contrac{p}_x': [] for p in CR_percents}  \n",
    "    contrac_y = {f'contrac{p}_y': [] for p in CR_percents}\n",
    "\n",
    "    for i in range(len(peaks)):\n",
    "        peak_value = strain_vector[peaks[i]]\n",
    "        trough_value = strain_vector[troughs[i]]\n",
    "        pretrough_value = strain_vector[pretroughs[i]]\n",
    "                \n",
    "        # Calculate target values for each contrac/relax percentage\n",
    "        targets_contrac = {f'target_{p}_on': (p/100)*(peak_value-pretrough_value)+pretrough_value for p in CR_percents}\n",
    "        targets_relax = {f'target_{p}': peak_value - (p/100)*(peak_value-trough_value) for p in CR_percents}\n",
    "        \n",
    "                \n",
    "        for p in CR_percents:\n",
    "            crossings_off = []\n",
    "            crossings_on = []\n",
    "            target_contrac = targets_contrac[f'target_{p}_on']\n",
    "            target_relax = targets_relax[f'target_{p}']\n",
    "            \n",
    "            #Relaxation      \n",
    "            for j in range(peaks[i], troughs[i]): # find all crossings between peak and trough\n",
    "                if (strain_vector[j-1] >= target_relax >= strain_vector[j]) or (strain_vector[j-1] <= target_relax <= strain_vector[j]):\n",
    "                    crossings_off.append(j)\n",
    "\n",
    "            #Contraction        \n",
    "            for j in range(pretroughs[i], peaks[i]): # find all crossings between pretrough and peak\n",
    "                if (strain_vector[j-1] <= target_contrac <= strain_vector[j]) or (strain_vector[j-1] >= target_contrac >= strain_vector[j]):\n",
    "                    crossings_on.append(j)\n",
    "                    \n",
    "            # Store the results if crossings were found\n",
    "            if crossings_off:\n",
    "                last_idx = crossings_off[-1]\n",
    "                off_duration = time_vector[last_idx] - time_vector[peaks[i]]\n",
    "                relax[f'relax{p}'].append(off_duration)\n",
    "                relax_y[f'relax{p}_y'].append(strain_vector[last_idx])\n",
    "                relax_x[f'relax{p}_x'].append(time_vector[last_idx])\n",
    "                    \n",
    "            if crossings_on:\n",
    "                first_idx = crossings_on[0]\n",
    "                contrac[f'contrac{p}'].append(time_vector[first_idx]-time_vector[pretroughs[i]])\n",
    "                contrac_x[f'contrac{p}_x'].append(time_vector[first_idx])\n",
    "                contrac_y[f'contrac{p}_y'].append(strain_vector[first_idx])\n",
    "\n",
    "    return relax, relax_x, relax_y, contrac, contrac_x, contrac_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for analysis\n",
    "Initial_Youngs_Modulus=11\n",
    "Days_Culture=14\n",
    "Youngs_Modulus=-0.3366*Days_Culture+Initial_Youngs_Modulus\n",
    "\n",
    "# PARAMETERS USED TO FILTER PEAKS OUT\n",
    "PRCT_1 = 10\n",
    "PRCT_2 = 40\n",
    "MIN_EXPANSION_FLOW = 0 # in pixels**2 * seconds\n",
    "WINDOW_SLOPE_SIZE = 50\n",
    "MIN_SLOPE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "# Selecting folder to analyse (folder containing all conditions)\n",
    "folder = input(\"Enter the path to the main folder: \")\n",
    "\n",
    "\n",
    "conditions = [f for f in os.listdir(folder) if os.path.isdir(os.path.join(folder, f)) and not f.startswith('Results')] # get only folders in folder except Results folder\n",
    "if len(conditions)==0 :\n",
    "    print('WARNING: No conditions found in folder. Exiting...')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing condition: Analysis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 446\u001b[0m\n\u001b[0;32m    443\u001b[0m         signal_to_save\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcondition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_signal.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# save the signal as CSVs\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# Save params as one csv for whole condition\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     aggregated_results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcondition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mname\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_param_means.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll signals from condition \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mcondition\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m processed!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll conditions processed!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loop through all folders/conditions in folder\n",
    "'''\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    print(\"Analyzing condition: \"+ condition)\n",
    "    path = f'{folder}/{condition}'\n",
    "    files = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('raw_data.csv') and 'aggregated' not in file:\n",
    "            file = os.path.join(path, file)\n",
    "            files.append(file) # list of all files in folder condition\n",
    "\n",
    "    # Create Results folder if it doesn't exist\n",
    "    if not os.path.exists(f'{folder}/Results'):\n",
    "        os.makedirs(f'{folder}/Results')\n",
    "    if not os.path.exists(f'{folder}/Results/{condition}'):\n",
    "        os.makedirs(f'{folder}/Results/{condition}')\n",
    "\n",
    "    # Initializing dataframe to store results \n",
    "    CR_percents = [10,50, 90]\n",
    "    columns = ['Name', 'Total video time','SNR','Initial Youngs Modulus', 'Days in culture','Period (s)', 'Beating Frequency (Hz)', 'Interpeak irregularity (s)', 'N_twitch','Contraction strain (A.U.)','Maximum contraction speed (A.U./s)','Maximum relaxation speed (A.U./s)', 'Contraction stress (mN/mm2)','Contraction time (s)', 'Relaxation time (s)', 'Contraction-relaxation time (s)']\n",
    "    for p in CR_percents:\n",
    "        columns.append(f'Contraction time {p} (s)')\n",
    "        columns.append(f'Relaxation time {p} (s)')\n",
    "    aggregated_results = pd.DataFrame(index=range(len(files)), columns=columns)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Loop through all files in folder/condition and analyses them\n",
    "    '''\n",
    "    for f in range(len(files)):\n",
    "        name = os.path.basename(files[f]).replace('.csv', '')  # whole file name without extension\n",
    "        # name = name.split('_', 1)[1] # TODO: remove this after modification in java code, right now it removes \"rawresults_\" in front of name\n",
    "        name_condition = name.rsplit('_', 1)[0] # name and condition without well and ring number\n",
    "        print('Processing file: ', name)\n",
    "\n",
    "\n",
    "        ''' 1. Load and plot raw data '''\n",
    "        raw_data = pd.read_csv(files[f])\n",
    "\n",
    "        # radius_column = next(col for col in raw_data.columns if col.startswith('Internal radius')) # find column with 'Mean' in its name\n",
    "        # radius_signal=raw_data[[radius_column]].values.flatten()\n",
    "\n",
    "        # time_column = next(col for col in raw_data.columns if col.startswith('Time')) # convert time index to seconds\n",
    "        # time_signal=raw_data[[time_column]].values.flatten()\n",
    "\n",
    "\n",
    "\n",
    "        radius_column = next(col for col in raw_data.columns if col.startswith('Internal radius')) # find column with 'Mean' in its name\n",
    "        radius_signal=raw_data[[radius_column]].values.flatten()\n",
    "\n",
    "        time_column = next(col for col in raw_data.columns if col.startswith('Time')) # convert time index to seconds\n",
    "        time_signal=raw_data[[time_column]].values.flatten()\n",
    "\n",
    "        #calculate strain and stress signals\n",
    "        max_radius=np.max(radius_signal)\n",
    "        strain_signal = ((max_radius - radius_signal) / max_radius)\n",
    "        stress_signal = strain_signal * Youngs_Modulus\n",
    "\n",
    "        ''' 2. Basic video parameters '''\n",
    "        deltaT=np.mean(np.diff(time_signal))\n",
    "        frame_rate=1/(deltaT)\n",
    "\n",
    "        video_time=time_signal[-1]\n",
    "        \n",
    "\n",
    "        # plt.figure(figsize=(15, 4)) # plot raw data\n",
    "        # plt.plot(time_signal, strain_signal)\n",
    "        # plt.xlabel('Time (s)')\n",
    "        # plt.ylabel('Strain')\n",
    "        # plt.title('Raw plot '+name) \n",
    "        # plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "        '''3. Data interpolation '''\n",
    "        n_points = 10000  # points in interpolated signal (a bit arbitrary maybe)\n",
    "        interp_func = interp1d(time_signal, strain_signal, kind='quadratic')\n",
    "        interp_func2 = interp1d(time_signal, radius_signal, kind='quadratic')\n",
    "        time_interp = np.linspace(np.min(time_signal), np.max(time_signal), n_points)   # create a new time array with more points\n",
    "        mean_interp = interp_func(time_interp) # interpolation\n",
    "        signal_interp = pd.DataFrame() # store all interpolated signals in new dataframe (they have different lengths)\n",
    "        signal_interp['Time (s)'] = time_interp\n",
    "        signal_interp['Radius'] = interp_func2(time_interp)\n",
    "        signal_interp['Strain'] = mean_interp\n",
    "\n",
    "        Total_time=time_interp[-1]\n",
    "                \n",
    "\n",
    "        strain_interp_signal=signal_interp[['Strain']].values.flatten()\n",
    "\n",
    "        frame_rate_interp=1/(video_time/len(strain_interp_signal))\n",
    "\n",
    "        '''4. Preliminary frequency fft '''\n",
    "\n",
    "        # --- Butterworth filter ---\n",
    "        Fs = frame_rate_interp  # Sampling frequency\n",
    "        fc = 5 # Cut-off frequency (Hz)\n",
    "\n",
    "        # Design 2nd order Butterworth low-pass filter\n",
    "        b, a = butter(2, fc / (Fs / 2), btype='low')  # Fs/2 is the Nyquist frequency\n",
    "\n",
    "        Filtered_strain_signal=filtfilt(b,a,strain_interp_signal)\n",
    "\n",
    "        # --- Frequency analysis using FFT ---\n",
    "        signal = Filtered_strain_signal\n",
    "        L = len(signal)\n",
    "        Y = fft(signal)\n",
    "        fftfreq = (frame_rate_interp) * np.arange(0, L // 2 + 1) / L\n",
    "\n",
    "        # FFT amplitude\n",
    "        P2 = np.abs(Y / L)\n",
    "        P1 = P2[:L // 2 + 1]\n",
    "        P1[1:-1] = 2 * P1[1:-1]  # Multiply by 2 (except first and last) for symmetry\n",
    "\n",
    "        # Find FFT peaks\n",
    "        fftpeaks_index,_= find_peaks(P1)\n",
    "\n",
    "        # plt.figure(figsize=(15, 4)) # plot raw data\n",
    "        # plt.plot(P1)\n",
    "        # plt.plot(fftpeaks_index,P1[fftpeaks_index],'*')\n",
    "        # plt.xlabel('Index')\n",
    "        # plt.ylabel('fft')\n",
    "        # plt.show()\n",
    "\n",
    "        # Get peak with max amplitude\n",
    "        fftmax_index = np.argmax(P1[fftpeaks_index])\n",
    "        freqfromfft = fftfreq[fftpeaks_index[fftmax_index]]\n",
    "        print(freqfromfft)\n",
    "\n",
    "\n",
    "        ''' 5. Peak detection '''\n",
    "\n",
    "        #Signal to noise ratio\n",
    "        SNR=signaltonoise(strain_interp_signal)\n",
    "        strain_interp_signal_old=strain_interp_signal\n",
    "        strain_interp_signal=Filtered_strain_signal\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # if SNR>2.5:\n",
    "        #     Noise=\"Yes\"\n",
    "        #     print('Signal too noisy, SNR= %f ' %SNR)\n",
    "        # else:\n",
    "        #     Noise=\"No\"\n",
    "\n",
    "        # match Noise:\n",
    "            \n",
    "            # case \"Yes\":\n",
    "\n",
    "            #     period=None\n",
    "            #     frequency=None\n",
    "            #     interpeak_std = None\n",
    "            #     n_twitch=None\n",
    "            #     mean_strain=None\n",
    "            #     Max_Contraction_speed=None\n",
    "            #     Max_Relaxation_speed=None\n",
    "            #     stress=None\n",
    "            #     Contraction_time=None\n",
    "            #     Relaxation_time=None\n",
    "            #     contraction_relaxation=None\n",
    "\n",
    "            #     results_dict ={\n",
    "            #         'Name':name,\n",
    "            #         'Total video time': Total_time,\n",
    "            #         'SNR':SNR,\n",
    "            #         'Initial Youngs Modulus': Initial_Youngs_Modulus,\n",
    "            #         'Days in culture': Days_Culture,\n",
    "            #         'Period (s)': period,\n",
    "            #         'Beating Frequency (Hz)': frequency,\n",
    "            #         'Interpeak irregularity (s)': interpeak_std,\n",
    "            #         'N_twitch': n_twitch,\n",
    "            #         'Contraction strain (A.U.)':mean_strain,\n",
    "            #         'Maximum contraction speed (A.U./s)':Max_Contraction_speed,\n",
    "            #         'Maximum relaxation speed (A.U./s)': Max_Relaxation_speed,\n",
    "            #         'Contraction stress (mN/mm2)': stress,\n",
    "            #         'Contraction time (s)': Contraction_time,\n",
    "            #         'Relaxation time (s)': Relaxation_time,\n",
    "            #         'Contraction-relaxation time (s)':contraction_relaxation}\n",
    "                \n",
    "            #     for p in CR_percents:  # add APD metrics using apd_percents list\n",
    "            #         results_dict[f'CT{p} (s)'] = None\n",
    "            #         results_dict[f'RT{p} (s)'] = None\n",
    "                \n",
    "            #     plt.figure(figsize=(15, 4))\n",
    "            #     plt.plot(signal_interp['Time (s)'], strain_interp_signal, label='Strain Signal')\n",
    "            #     plt.savefig(f'{folder}/Results/{condition}/{name}_signal.png', facecolor = 'w', bbox_inches=\"tight\", pad_inches=0.3)\n",
    "            #     plt.show()\n",
    "                \n",
    "            #     aggregated_results.iloc[f] = pd.Series(results_dict)\n",
    "            #     signal_to_save = signal_interp[['Time (s)', 'Radius','Strain']]\n",
    "            #     signal_to_save.columns = ['Time (s)', 'Radius (pixels)','Strain (A.U.)'] \n",
    "            #     signal_to_save.insert(0, 'Name', name)\n",
    "            #     signal_to_save.to_csv(f'{folder}/Results/{condition}/{name}_signal.csv', index=False) # save the signal as CSVs\n",
    "\n",
    "\n",
    "            \n",
    "            # case 'No':\n",
    "                # Quantacell Peak detection\n",
    "\n",
    "        dStrain=np.diff(strain_interp_signal)\n",
    "\n",
    "\n",
    "        # MIN_SLOPE=np.mean(dStrain[dStrain>0]*0.1)\n",
    "        # MIN_SLOPE=0\n",
    "        # MIN_DURATION=0\n",
    "        MIN_DISTANCE=1/freqfromfft/5*frame_rate_interp\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # results = analyze_signal_peaks(strain_interp_signal,\n",
    "        #                                     prct_1=PRCT_1, prct_2=PRCT_2,\n",
    "        #                                     sample_rate=frame_rate_interp,\n",
    "        #                                     min_duration=MIN_DURATION,\n",
    "        #                                     min_distance=MIN_DISTANCE,\n",
    "        #                                     min_flow=MIN_EXPANSION_FLOW,\n",
    "        #                                     min_slope=MIN_SLOPE,\n",
    "        #                                     window_slope_size=WINDOW_SLOPE_SIZE,\n",
    "        #                                     verbose=True)\n",
    "\n",
    "        # all_peaks=np.asarray(results[\"high_peak_indices\"])\n",
    "\n",
    "        MinHeight=PRCT_2/100*np.max(strain_interp_signal)\n",
    "        all_peaks,_=find_peaks(strain_interp_signal,height=MinHeight,distance=MIN_DISTANCE)\n",
    "\n",
    "        # plot_signal_with_filtered_peaks(strain_signal, frame_rate, results)\n",
    "\n",
    "        \n",
    "        MaxStrain=np.max(strain_interp_signal)\n",
    "        MinStrain=np.min(strain_interp_signal)\n",
    "\n",
    "        if len(all_peaks)==0:\n",
    "            continue\n",
    "\n",
    "        max_trough = (MaxStrain-MinStrain)*0.5 + MinStrain\n",
    "        all_troughs, _ = find_peaks(-strain_interp_signal, height = (-max_trough, None))\n",
    "        # all_troughs, _ = find_peaks(-strain_interp_signal)  \n",
    "\n",
    "        # plt.figure(figsize=(15, 4))\n",
    "        # plt.plot(time_interp, strain_interp_signal, label='Strain Signal')\n",
    "        # plt.plot(time_interp[all_peaks], strain_interp_signal[all_peaks],'*', label='Peaks')\n",
    "        # plt.plot(time_interp[pretroughs], strain_interp_signal[pretroughs],'*',color='red', label='pretroughs')\n",
    "        # plt.plot(time_interp[all_troughs], strain_interp_signal[all_troughs],'*',color='green', label='troughs')\n",
    "\n",
    "\n",
    "\n",
    "        # print('test=%f'%all_peaks[-1])\n",
    "        peaks, pretroughs, troughs, passs, message = sort_peaks_troughs(strain_interp_signal,all_peaks, all_troughs,50)\n",
    "        if passs:\n",
    "            print(message)\n",
    "            period=None\n",
    "            frequency=None\n",
    "            interpeak_std = None\n",
    "            if len(all_peaks)>0:\n",
    "                n_twitch=len(all_peaks)\n",
    "            else:\n",
    "                n_twitch=None                \n",
    "            mean_strain=None                    \n",
    "            Max_Contraction_speed=None\n",
    "            Max_Relaxation_speed=None\n",
    "            stress=None\n",
    "            Contraction_time=None\n",
    "            Relaxation_time=None\n",
    "            contraction_relaxation=None\n",
    "\n",
    "            results_dict ={\n",
    "                'Name':name,\n",
    "                'Total video time': Total_time,\n",
    "                'SNR':SNR,\n",
    "                'Initial Youngs Modulus': Initial_Youngs_Modulus,\n",
    "                'Days in culture': Days_Culture,\n",
    "                'Period (s)': period,\n",
    "                'Beating Frequency (Hz)': frequency,\n",
    "                'Interpeak irregularity (s)': interpeak_std,\n",
    "                'N_twitch': n_twitch,\n",
    "                'Contraction strain (A.U.)':mean_strain,\n",
    "                'Maximum contraction speed (A.U./s)':Max_Contraction_speed,\n",
    "                'Maximum relaxation speed (A.U./s)': Max_Relaxation_speed,\n",
    "                'Contraction stress (mN/mm2)': stress,\n",
    "                'Contraction time (s)': Contraction_time,\n",
    "                'Relaxation time (s)': Relaxation_time,\n",
    "                'Contraction-relaxation time (s)':contraction_relaxation}\n",
    "            \n",
    "            for p in CR_percents:  # add APD metrics using apd_percents list\n",
    "                results_dict[f'CT{p} (s)'] = None\n",
    "                results_dict[f'RT{p} (s)'] = None\n",
    "            \n",
    "            plt.figure(figsize=(15, 4))\n",
    "            plt.plot(signal_interp['Time (s)'], strain_interp_signal, label='Strain Signal')\n",
    "            plt.savefig(f'{folder}/Results/{condition}/{name}_signal.png', facecolor = 'w', bbox_inches=\"tight\", pad_inches=0.3)\n",
    "            # plt.show()\n",
    "        \n",
    "            aggregated_results.iloc[f] = pd.Series(results_dict)\n",
    "            signal_to_save = signal_interp[['Time (s)', 'Radius','Strain']]\n",
    "            signal_to_save.columns = ['Time (s)', 'Radius (pixels)','Strain (A.U.)'] \n",
    "            signal_to_save.insert(0, 'Name', name)\n",
    "            signal_to_save.to_csv(f'{folder}/Results/{condition}/{name}_signal.csv', index=False) # save the signal as CSVs\n",
    "\n",
    "            continue\n",
    "\n",
    "        # plt.figure(figsize=(15, 4))\n",
    "        # plt.plot(time_interp, strain_interp_signal, label='Strain Signal')\n",
    "        # plt.plot(time_interp[peaks], strain_interp_signal[peaks],'*', label='Peaks')\n",
    "        # plt.plot(time_interp[pretroughs], strain_interp_signal[pretroughs],'*',color='red', label='pretroughs')\n",
    "        # plt.plot(time_interp[troughs], strain_interp_signal[troughs],'*',color='green', label='troughs')\n",
    "\n",
    "        # plt.figure(figsize=(15, 4))\n",
    "        # plt.plot(time_interp[1:], dStrain, label='Strain Signal')\n",
    "        # plt.plot(time_interp[peaks], dStrain[peaks],'*', label='Peaks')\n",
    "        # plt.plot(time_interp[pretroughs], dStrain[pretroughs],'*',color='red', label='pretroughs')\n",
    "        # plt.plot(time_interp[troughs], dStrain[troughs],'*',color='green', label='troughs')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        ''' 6. Derivative of the signal '''\n",
    "        dt=np.mean(np.diff(time_interp))\n",
    "        dStrain_dt=dStrain/dt\n",
    "\n",
    "        MaxSpeedHeight=np.max(dStrain_dt)*0.4\n",
    "        MinSpeedHeight=np.max(-dStrain_dt)*0.3\n",
    "        if len(peaks)>1:\n",
    "            MinDistance=np.mean(np.diff(time_interp[peaks]))*frame_rate_interp*0.3 #at least 1/3 of the period (indices)\n",
    "        else:\n",
    "            MinDistance=0\n",
    "\n",
    "        speed_max_peaks,_=find_peaks(dStrain_dt,height=MaxSpeedHeight,distance=MinDistance)\n",
    "        speed_min_peaks,_=find_peaks(-dStrain_dt,height=MinSpeedHeight,distance=MinDistance)\n",
    "\n",
    "        # plt.figure(figsize=(15, 4)) # plot raw data\n",
    "        # plt.plot(time_interp[1:],dStrain_dt)\n",
    "        # plt.plot(time_interp[speed_max_peaks], dStrain_dt[speed_max_peaks],'*',color='red', label='Max')\n",
    "        # plt.plot(time_interp[speed_min_peaks], dStrain_dt[speed_min_peaks],'*', color='green',label='Min')\n",
    "        # plt.plot(time_interp[pretroughs], dStrain_dt[pretroughs],'*',color='red', label='pretroughs')\n",
    "        # plt.plot(time_interp[troughs], dStrain_dt[troughs],'*', color='green',label='troughs')\n",
    "        # plt.xlabel('Time (s)')\n",
    "        # plt.ylabel('Velocity')\n",
    "        # plt.show()\n",
    "\n",
    "        ''' 7. Compute main metrics here '''\n",
    "        interpeak_distance = np.diff(time_interp[peaks])\n",
    "        period = np.mean(interpeak_distance) \n",
    "        frequency=1/period\n",
    "        interpeak_std = np.std(interpeak_distance) # will be None if only 2 peaks\n",
    "        n_twitch=len(peaks)\n",
    "\n",
    "        strain_maxima = np.array(strain_interp_signal[peaks]) - np.array(strain_interp_signal[pretroughs])\n",
    "        mean_strain = np.mean(strain_maxima)\n",
    "        std_strain = np.std(strain_maxima)\n",
    "        stress=mean_strain * Youngs_Modulus\n",
    "\n",
    "\n",
    "        Max_Contraction_speed = np.mean(dStrain_dt[speed_max_peaks])\n",
    "        Max_Relaxation_speed = abs(np.mean(dStrain_dt[speed_min_peaks]))\n",
    "\n",
    "\n",
    "        Contraction_time = np.mean(time_interp[peaks] - time_interp[pretroughs])\n",
    "        Relaxation_time=np.mean(time_interp[troughs] - time_interp[peaks])\n",
    "        contraction_relaxation = np.mean(time_interp[troughs]- time_interp[pretroughs]) # time between pretrough and trough = contraction-relaxation\n",
    "\n",
    "        ''' 8. Compute contraction and relaxation times '''\n",
    "\n",
    "        relax, relax_x, relax_y, contrac, contrac_x, contrac_y = get_ContractionPoints(i, CR_percents, peaks,strain_interp_signal,time_interp, pretroughs, troughs) # compute APD values for each action potential\n",
    "\n",
    "            # Plot the results on whole signal\n",
    "        plt.figure(figsize=(13, 3.5))\n",
    "        plt.plot(time_interp, strain_interp_signal, label='Mean Signal')\n",
    "        plt.scatter(time_interp[peaks], strain_interp_signal[peaks], color='red', label='Peaks')\n",
    "        plt.scatter(time_interp[troughs], strain_interp_signal[troughs], color='green', label='Ends of twitch')\n",
    "        plt.scatter(time_interp[pretroughs], strain_interp_signal[pretroughs], color='blue', label='Starts of twitch')\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(CR_percents))) # plot APD points with different colors\n",
    "        for (p, color) in zip(CR_percents, colors):\n",
    "            plt.scatter(relax_x[f'relax{p}_x'], relax_y[f'relax{p}_y'], \n",
    "                        color=color, label=f'RT{p}', marker='o', alpha=0.7)\n",
    "            plt.scatter(contrac_x[f'contrac{p}_x'], contrac_y[f'contrac{p}_y'], \n",
    "                        color=color,label=f'CT{p}', marker='d', alpha=0.7)\n",
    "        plt.title(\"Signal with CR values\")\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Strain (a.u.)')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{folder}/Results/{condition}/{name}_plot.png', facecolor = 'w', bbox_inches=\"tight\", pad_inches=0.3)\n",
    "        # plt.show()\n",
    "\n",
    "        print(f'Max unfiltered={max(strain_interp_signal_old)}')\n",
    "        print(f'Max filtered={max(strain_interp_signal)}')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        ''' 8. Save signal and metrics '''\n",
    "    \n",
    "        # add param means to results_dict\n",
    "        results_dict ={\n",
    "            'Name':name,\n",
    "            'Total video time': Total_time,\n",
    "            'SNR':SNR,\n",
    "            'Initial Youngs Modulus': Initial_Youngs_Modulus,\n",
    "            'Days in culture': Days_Culture,\n",
    "            'Period (s)': period,\n",
    "            'Beating Frequency (Hz)': frequency,\n",
    "            'Interpeak irregularity (s)': interpeak_std,\n",
    "            'N_twitch': n_twitch,\n",
    "            'Contraction strain (A.U.)':mean_strain,\n",
    "            'Maximum contraction speed (A.U./s)':Max_Contraction_speed,\n",
    "            'Maximum relaxation speed (A.U./s)': Max_Relaxation_speed,\n",
    "            'Contraction stress (mN/mm2)': stress,\n",
    "            'Contraction time (s)': Contraction_time,\n",
    "            'Relaxation time (s)': Relaxation_time,\n",
    "            'Contraction-relaxation time (s)':contraction_relaxation}\n",
    "\n",
    "\n",
    "        for p in CR_percents:  # add APD metrics using apd_percents list\n",
    "            results_dict[f'CT{p} (s)'] = np.mean(contrac[f'contrac{p}'])\n",
    "            results_dict[f'RT{p} (s)'] = np.mean(relax[f'relax{p}'])\n",
    "\n",
    "        aggregated_results.iloc[f] = pd.Series(results_dict)  # add results dict as a line of aggregated_results dataframe without using append\n",
    "\n",
    "        StrainPoints = pd.DataFrame({**contrac, **relax, **relax_x, **relax_y, **contrac_x, **contrac_y})\n",
    "        StrainPoints.insert(0, 'Name', name)\n",
    "        StrainPoints.insert(1, 'Pretroughs', pretroughs)\n",
    "        StrainPoints.insert(2, 'Pretroughs_x', np.array(time_interp[pretroughs]))\n",
    "        StrainPoints.insert(3, 'Pretroughs_y', np.array(strain_interp_signal[pretroughs]))\n",
    "        StrainPoints.insert(4, 'Peaks', peaks)\n",
    "        StrainPoints.insert(5, 'Peaks_x', np.array(time_interp[peaks]))\n",
    "        StrainPoints.insert(6, 'Peaks_y', np.array(strain_interp_signal[peaks]))\n",
    "        StrainPoints.insert(7, 'Troughs', troughs)\n",
    "        StrainPoints.insert(8, 'Troughs_x', np.array(time_interp[troughs]))\n",
    "        StrainPoints.insert(9, 'Troughs_y', np.array(strain_interp_signal[troughs]))\n",
    "        StrainPoints.to_csv(f'{folder}/Results/{condition}/{name}_ContractionPoints.csv', index=False) # save CT and CR points as CSVs\n",
    "\n",
    "        signal_to_save = signal_interp[['Time (s)', 'Radius','Strain']]\n",
    "        signal_to_save.columns = ['Time (s)', 'Radius (pixels)','Strain (A.U.)'] \n",
    "        signal_to_save.insert(0, 'Name', name)\n",
    "        signal_to_save.to_csv(f'{folder}/Results/{condition}/{name}_signal.csv', index=False) # save the signal as CSVs\n",
    "\n",
    "    # Save params as one csv for whole condition\n",
    "    aggregated_results.to_csv(f'{folder}/Results/{condition}/{name}_param_means.csv', index=False)\n",
    "\n",
    "    print('All signals from condition '+condition+' processed!\\n\\n\\n')\n",
    "\n",
    "print('All conditions processed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
